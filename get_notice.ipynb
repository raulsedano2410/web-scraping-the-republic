{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import requests  # For making HTTP requests\n",
    "import csv  # For reading and writing CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import time  # For pauses between requests\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# By modifying the URL to access the news, we save the new links\n",
    "# the new file must end in: _approved_pattern.csv\n",
    "\n",
    "def separate_by_pattern(file_path):\n",
    "\n",
    "    url_pattern = \"https://quotidiano.repubblica.it/edicola/searchdetail\\?id=http://archivio.repubblica.extra.kataweb.it/archivio/repubblica/\"\n",
    "\n",
    "    # pattern to follow\n",
    "    new_url_base = \"https://ricerca.repubblica.it/repubblica/archivio/repubblica/\"\n",
    "\n",
    "\n",
    "    outputfolder = \"output\"\n",
    "\n",
    "    # # absolute path\n",
    "    # file_path = os.path.join(os.getcwd(),outputfolder, file)\n",
    "    # file_path = os.path.abspath(file_path)\n",
    "\n",
    "    # create file name\n",
    "    output_file = os.path.splitext(file_path)[0] + \"_approved_pattern.csv\"\n",
    "\n",
    "    if not os.path.exists(output_file):\n",
    "\n",
    "        # Read the CSV file\n",
    "        df_src = pd.read_csv(file_path)\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # Filter rows containing the pattern in the 'link' column\n",
    "        rows_with_pattern = df_src[df_src[\"link\"].str.contains(url_pattern, na=False)]\n",
    "\n",
    "        # Generate new column\n",
    "        # df_src[\"notice_link\"] = \"\"\n",
    "\n",
    "        for index, row in rows_with_pattern.iterrows():\n",
    "\n",
    "            link = row[\"link\"]\n",
    "\n",
    "            # Quitar patr√≥n y eemplazar base\n",
    "            notice_link = re.sub(url_pattern, new_url_base, link)\n",
    "\n",
    "              # Agregar a la lista como diccionario\n",
    "            rows.append({\"link\": notice_link})\n",
    "\n",
    "        df_final = pd.DataFrame(rows)\n",
    "\n",
    "        # Save the filtered rows to a new CSV file\n",
    "        df_final.to_csv(output_file, index=False)\n",
    "\n",
    "        # Create the .txt file with the details\n",
    "        details_file = f\"details_{os.path.basename(output_file)}.txt\"\n",
    "        with open(os.path.abspath(os.path.join(outputfolder,details_file)), \"w\") as f:\n",
    "            f.write(f\"Total files: {len(rows_with_pattern)}\\n\")\n",
    "            f.write(f\"Rows in original file: {len(df_src)}\\n\")\n",
    "            f.write(f\"Rows with pattern: {len(rows_with_pattern)}\\n\")\n",
    "            f.write(f\"Rows without pattern: {len(df_src) - len(rows_with_pattern)}\\n\")\n",
    "\n",
    "        # Print the name of the output file\n",
    "        print(f'The {os.path.basename(output_file)} file was created successfully')\n",
    "\n",
    "\n",
    "        print(f'The {os.path.basename(output_file)} file was created succesfully')\n",
    "        print(f\"CHeck in : {output_file}\")\n",
    "\n",
    "    else:\n",
    "        # Print the number of rows found\n",
    "        print(f\"The {os.path.basename(output_file)} file already exists.\")\n",
    "        print(f\"CHeck in : {output_file}\")\n",
    "\n",
    "    return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(response, src_filename):\n",
    "    # *** *** Parse the HTML content *** ***\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    article = soup.find(\"article\")\n",
    "\n",
    "    title = article.find(\"h1\").get_text(separator=\" \", strip=True)\n",
    "    title = title or \"No title\"\n",
    "    # Extract the article\n",
    "    content_tag = article.find(\"p\")\n",
    "\n",
    "    if content_tag is not None:\n",
    "        content = content_tag.get_text()\n",
    "    else:\n",
    "        content = \"\"\n",
    "\n",
    "    date = (\n",
    "        article.find(\"aside\").find(\"a\").find(\"time\").get_text(separator=\" \", strip=True)\n",
    "    )\n",
    "    # *** *** End parse *** ***\n",
    "\n",
    "    # objective data\n",
    "    article_data = [\n",
    "        {\"title\": title, \"content\": content, \"date\": date, \"src_url\": response.url}\n",
    "    ]\n",
    "\n",
    "    # Defines the field names\n",
    "    fieldnames = [\"title\", \"content\", \"date\", \"src_url\"]\n",
    "\n",
    "    # output name\n",
    "    name_output = src_filename.split(\"_\")[0] + \"_output_notice.csv\"\n",
    "\n",
    "    # Check if output file exists\n",
    "    mode = \"a\" if os.path.exists(name_output) else \"w\"\n",
    "    # If file exists, open in append mode, else create (write mode)\n",
    "    with open(name_output, mode, newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        # If opened in write mode (new file), write headers\n",
    "        if mode == \"w\":\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write article data to CSV file\n",
    "        # writerows takes a list of dictionaries and writes each\n",
    "        # as a row, using the specified headers\n",
    "        writer.writerows(article_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(filename):\n",
    "    # files filtered by pattern\n",
    "    approved_url_route = separate_by_pattern(filename)\n",
    "\n",
    "\n",
    "    # I get the absolute path depending on the operating system.\n",
    "    # outputfolder = \"output\"\n",
    "    backup_filepath = filename.split(\"_\")[0] + \"_backup_links.csv\"\n",
    "    # backup_filepath = os.path.join(os.getcwd(),outputfolder, backupfile)\n",
    "\n",
    "    if not os.path.exists(backup_filepath):\n",
    "        with open(backup_filepath, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=['notice_link'])\n",
    "            writer.writeheader()\n",
    "\n",
    "    # backup\n",
    "    df_backup = pd.read_csv(backup_filepath)\n",
    "\n",
    "    # Open the CSV file\n",
    "    df = pd.read_csv(approved_url_route)\n",
    "\n",
    "    urls_to_skip = set(df_backup.notice_link)\n",
    "\n",
    "    # Iterate over each row of data\n",
    "    for iteration, row in df.iterrows():\n",
    "\n",
    "        # if \"link\" not in row or pd.isna(row[\"link\"]):\n",
    "        #     # Skip rows where notice_link does not exist or not NA\n",
    "        #     continue\n",
    "        link = row[\"link\"]  # Gets the link from the 'link' column\n",
    "\n",
    "        if link not in urls_to_skip:\n",
    "\n",
    "            print(f'iteraton in row # {iteration}')\n",
    "\n",
    "            # Scrape with BeautifulSoup\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "\n",
    "                # Extract data from the website here\n",
    "                parse(response, approved_url_route  )\n",
    "\n",
    "                # Check if the file exists\n",
    "                mode = \"a\" if os.path.exists(backup_filepath) else \"w\"\n",
    "                # If the file exists, open it in append mode.\n",
    "                with open(backup_filepath, 'a', newline=\"\", encoding=\"utf-8\") as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=['notice_link'], extrasaction=\"ignore\")\n",
    "                    if mode == \"w\":\n",
    "                        writer.writeheader()\n",
    "                    # Write the news and its metadata to the file\n",
    "                    writer.writerow({\"notice_link\":link})\n",
    "\n",
    "            elif response.status_code == 403:\n",
    "\n",
    "                print('status is 403\\n' * 3)\n",
    "\n",
    "                print('start the 4 minute wait')\n",
    "\n",
    "                # Wait for 4 minutes before retrying\n",
    "                for i in range(1,5):\n",
    "                    time.sleep(60)\n",
    "                    print(f'{i} minutes have passed')\n",
    "                print('restarting scraper based on backup')\n",
    "                # # fallback-based recursion\n",
    "                # scraper(filename)\n",
    "        else:\n",
    "            print(f\"row # {iteration} Link already processed, jumping...\")\n",
    "\n",
    "    print('Complete scraping')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
