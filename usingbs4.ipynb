{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: center;\">\n",
    "    <h2>Convert_date</h2>\n",
    "    <p>Is created to translate the Italian dates that we need into the format that the URL needs</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; justify-content: flex-end;\">\n",
    "    <img src=\"images/date_italian.png\" style=\"max-width: 80%; max-height: 80%; margin-left: auto; margin-right: auto;\" alt=\"Date Italian\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Date(italian_date):\n",
    "# Mapping Italian month names to month numbers\n",
    "    meses_italiano_a_numero = {\n",
    "        'gennaio': '01',\n",
    "        'febbraio': '02',\n",
    "        'marzo': '03',\n",
    "        'aprile': '04',\n",
    "        'maggio': '05',\n",
    "        'giugno': '06',\n",
    "        'luglio': '07',\n",
    "        'agosto': '08',\n",
    "        'settembre': '09',\n",
    "        'ottobre': '10',\n",
    "        'novembre': '11',\n",
    "        'dicembre': '12'\n",
    "    }\n",
    "\n",
    "    # Split the Italian date into parts\n",
    "    partes_fecha = italian_date.split()\n",
    "\n",
    "    # Get day, month and year\n",
    "    dia = partes_fecha[0]\n",
    "    mes = meses_italiano_a_numero[partes_fecha[1]]\n",
    "    año = partes_fecha[2]\n",
    "\n",
    "    #format the date in the desired format\n",
    "    fecha_formateada = f\"{año}-{mes}-{dia}\"\n",
    "\n",
    "    return fecha_formateada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: center;\">\n",
    "    <h2>create_csv</h2>\n",
    "    <p>Is responsible for creating CSV files with names according to the search entered in the main function, in this file will be all the links that will be scraped later</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; justify-content: flex-end;\">\n",
    "    <img src=\"images/csvs.png\" style=\"max-width: 100%; max-height: 100%; margin-left: auto; margin-right: auto;\" alt=\"CSVs\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to generate a CSV file with the scraped links\n",
    "def create_csv(keyword, from_date, to_date, the_urls):\n",
    "\n",
    "    # Format keyword for file naming\n",
    "    keyword = keyword.replace(\" \", \"_\")\n",
    "\n",
    "    # Construct CSV file path/name with dates\n",
    "    csv_file_path = '_'.join([keyword, from_date, to_date]) + '.csv'\n",
    "\n",
    "    # Create list of dicts from urls with header \"URL\"\n",
    "    data = [{\"URL\": url} for url in the_urls]\n",
    "\n",
    "    if os.path.exists(csv_file_path):\n",
    "\n",
    "        # File exists, open in append mode\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "\n",
    "            # Append new url data rows to existing file\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"Data added to existing file: {csv_file_path}\")\n",
    "    else:\n",
    "        # File does not exist, open in write mode\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "   \n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write url data rows\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\"New CSV file created: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "# Scrapes total number of pages from search results\n",
    "def get_the_total_number_of_pages(url):\n",
    "\n",
    "    # Issue GET request and parse HTML\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    root = html.fromstring(str(soup))\n",
    "\n",
    "    # XPath to extract total pages text\n",
    "    # Handles case where no results exist\n",
    "    try:\n",
    "        paragraph_text = root.xpath('//*[@id=\"lista-risultati\"]/div/p/text()[2]')[0]\n",
    "        total_pages = paragraph_text.split()[1]\n",
    "    except:\n",
    "        total_pages = 0\n",
    "\n",
    "    # Convert to integer and return\n",
    "    return int(total_pages)\n",
    "\n",
    "\n",
    "# Gets date of the most recent article result\n",
    "def get_date_of_next_period(url):\n",
    "\n",
    "    # Request page and parse HTML\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    root = html.fromstring(str(soup))\n",
    "\n",
    "    # XPath to extract last date in results set\n",
    "    dates = root.xpath('//*[@id=\"lista-risultati\"]/article/aside/a')[-1]\n",
    "    date = dates.text_content().strip()\n",
    "\n",
    "    # Return cleaned date string\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates list of search result page URLs\n",
    "def urls_generator(keyword, from_date, to_date, modality, number_of_pages):\n",
    "\n",
    "    # Format keyword string for URL\n",
    "    keyword = keyword.replace(\" \", \"+\")\n",
    "\n",
    "    # Base search results URL with placeholder params\n",
    "    url_base = \"https://ricerca.repubblica.it/ricerca/repubblica?query={}&fromdate={}&todate={}&sortby=adate&author=&mode={}&page={}\"\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    # Iterate through number of pages\n",
    "    for page in range(1, number_of_pages + 1):\n",
    "\n",
    "        # Populate base URL with parameters\n",
    "        # Page number is iterated\n",
    "        url = url_base.format(keyword, from_date, to_date, modality, page)\n",
    "\n",
    "        # Append to list of URLs\n",
    "        urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Main scraping function\n",
    "def scraper(keyword, from_date, to_date, modality):\n",
    "\n",
    "    # Generate initial set of search result URLs\n",
    "    urls = urls_generator(keyword, from_date, to_date, modality, 1)\n",
    "\n",
    "    # Get total pages for the search\n",
    "    total_number_of_pages = get_the_total_number_of_pages(urls[0])  # Example 140\n",
    "\n",
    "    # Handle no results case\n",
    "    if total_number_of_pages == 0:\n",
    "        return print(\"there is no news with the entry entered\")\n",
    "\n",
    "    # If < 50 pages, scrape all\n",
    "    elif total_number_of_pages <= 50:\n",
    "        the_urls = urls_generator(\n",
    "            keyword, from_date, to_date, modality, total_number_of_pages\n",
    "        )\n",
    "\n",
    "        # Save URLs to CSV\n",
    "        create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "    # If > 50 pages, scrape in batches\n",
    "    else:\n",
    "        max_pages_per_keyword = 50\n",
    "\n",
    "        # First batch of 50 URLs\n",
    "        the_urls = urls_generator(\n",
    "            keyword, from_date, to_date, modality, max_pages_per_keyword\n",
    "        )\n",
    "\n",
    "        # Save CSV\n",
    "        create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "        # Scrape in batches of 50 using date pagination\n",
    "        while True:\n",
    "\n",
    "            # Get next period start date\n",
    "            date_of_next_period_in_italian = get_date_of_next_period(the_urls[-1])\n",
    "            date_of_next_period = convert_Date(date_of_next_period_in_italian)\n",
    "            print(\"procces in \" + date_of_next_period)\n",
    "\n",
    "            # Generate single URL to get total pages\n",
    "            the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, 1)  # return array\n",
    "\n",
    "            # Get total pages for new date period range\n",
    "            total_number_of_pages = get_the_total_number_of_pages(the_urls[0])\n",
    "\n",
    "            # Handle total pages cases\n",
    "            if total_number_of_pages > 50:\n",
    "\n",
    "                # If more than 50 pages, generate batch\n",
    "                the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, max_pages_per_keyword)\n",
    "\n",
    "                create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "            elif total_number_of_pages <= 50:\n",
    "\n",
    "                # If less than 50, generate all pages\n",
    "                the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, total_number_of_pages)\n",
    "\n",
    "                create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo archivo CSV creado: mafia_nigeriana_2023-05-08_2024-01-01.csv\n",
      "procces in 2023-06-30\n",
      "Data added to existing file: mafia_nigeriana_2023-05-08_2024-01-01.csv\n",
      "procces in 2023-08-23\n",
      "Data added to existing file: mafia_nigeriana_2023-05-08_2024-01-01.csv\n",
      "procces in 2023-11-16\n",
      "Data added to existing file: mafia_nigeriana_2023-05-08_2024-01-01.csv\n",
      "*********** All existing urls of the keyword were saved END ***********\n",
      "*********** All existing urls of the keyword were saved END ***********\n",
      "*********** All existing urls of the keyword were saved END ***********\n"
     ]
    }
   ],
   "source": [
    "scraper(\"mafia nigeriana\", '2023-05-08', \"2024-01-01\", \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-08'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_Date('08 maggio 2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
