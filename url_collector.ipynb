{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script uses the following libraries:\n",
    "# - requests: for making HTTP requests (pip install requests)\n",
    "# - BeautifulSoup (bs4): for parsing HTML (pip install beautifulsoup4)\n",
    "# - lxml: for parsing HTML with BeautifulSoup (pip install lxml)\n",
    "# - csv: for reading and writing CSV files (part of Python's standard library)\n",
    "# - os: for interacting with the operating system (part of Python's standard library)\n",
    "\n",
    "# To install the necessary libraries, run the following commands in your terminal or command prompt:\n",
    "# pip install requests\n",
    "# pip install beautifulsoup4\n",
    "# pip install lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>convert_Date</h2>\n",
    "    <p>Translates Italian dates into the required URL format.</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; justify-content: flex-end;\">\n",
    "    <img src=\"images/date_italian.png\" style=\"max-width: 60%; max-height: 80%; margin-left: auto; margin-right: auto;\" alt=\"Date Italian\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Function Description\n",
    "\n",
    "Translate Italian dates into the format needed for URLs.\n",
    "\n",
    "**Parameters:**\n",
    "- *italian_date* (str): Italian date string to be translated.\n",
    "\n",
    "**Returns:** Translated date string in the format \"YYYY-MM-DD\".\n",
    "\n",
    "The function maps Italian month names to month numbers, extracts day, month, and year from the Italian date, and formats it into the desired format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Date(italian_date):\n",
    "# Mapping Italian month names to month numbers\n",
    "    meses_italiano_a_numero = {\n",
    "        'gennaio': '01',\n",
    "        'febbraio': '02',\n",
    "        'marzo': '03',\n",
    "        'aprile': '04',\n",
    "        'maggio': '05',\n",
    "        'giugno': '06',\n",
    "        'luglio': '07',\n",
    "        'agosto': '08',\n",
    "        'settembre': '09',\n",
    "        'ottobre': '10',\n",
    "        'novembre': '11',\n",
    "        'dicembre': '12'\n",
    "    }\n",
    "\n",
    "    # Split the Italian date into parts\n",
    "    partes_fecha = italian_date.split()\n",
    "\n",
    "    # Get day, month and year\n",
    "    dia = partes_fecha[0]\n",
    "    mes = meses_italiano_a_numero[partes_fecha[1]]\n",
    "    año = partes_fecha[2]\n",
    "\n",
    "    #format the date in the desired format\n",
    "    fecha_formateada = f\"{año}-{mes}-{dia}\"\n",
    "\n",
    "    return fecha_formateada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>create_csv</h2>\n",
    "    <p>Creates CSV files named after the search term. It contains all scraped links for later use.</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; justify-content: flex-end;\">\n",
    "    <img src=\"images/csvs.png\" style=\"max-width: 100%; max-height: 100%; margin-left: auto; margin-right: auto;\" alt=\"CSVs\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Function Description\n",
    "\n",
    "Create or add scraped links related to a specific search to a CSV file.\n",
    "\n",
    "**Parameters:**\n",
    "- *keyword* (str): Search term for the CSV file.\n",
    "- *from_date* (str): Start date of the search.\n",
    "- *to_date* (str): End date of the search.\n",
    "- *the_urls* (list): List of scraped links.\n",
    "\n",
    "**Returns:** None\n",
    "\n",
    "Generates a CSV file named after the search term and specified period. If the file exists, new links are added. If not, a new one is created. Links are written with a \"URL\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to generate a CSV file with the scraped links\n",
    "def create_csv(keyword, from_date, to_date, the_urls):\n",
    "\n",
    "    OUTPUT_DIR = 'output'\n",
    "\n",
    "    # Create output dir if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # Format keyword for file naming\n",
    "    keyword = keyword.replace(\" \", \"_\")\n",
    "\n",
    "    # Construct CSV file path/name with dates\n",
    "    csv_file_path = os.path.join(OUTPUT_DIR, '_'.join([keyword, from_date, to_date]) + '.csv')\n",
    "\n",
    "\n",
    "    # Create list of dicts from urls with header \"URL\"\n",
    "    data = [{\"URL\": url} for url in the_urls]\n",
    "\n",
    "    if os.path.exists(csv_file_path):\n",
    "\n",
    "        # File exists, open in reading mode to review content\n",
    "        with open(csv_file_path, mode='r', newline='') as file:\n",
    "            existing_data = list(csv.DictReader(file))\n",
    "\n",
    "        # Check if the new data is already in the existing file\n",
    "        existing_urls = set(row[\"URL\"] for row in existing_data)\n",
    "        new_urls = set(row[\"URL\"] for row in data)\n",
    "        if new_urls.issubset(existing_urls):\n",
    "            print(\"The new data already exists in the file.\")\n",
    "        else:\n",
    "\n",
    "            # File exists, open in append mode\n",
    "            with open(csv_file_path, mode='a', newline='') as file:\n",
    "\n",
    "                writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "\n",
    "                # Append new url data rows to existing file\n",
    "                for row in data:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        print(f\"Data added to existing file: {csv_file_path}\")\n",
    "    else:\n",
    "        # File does not exist, open in write mode\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write url data rows\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        print(f\"New CSV file created: {csv_file_path}\")\n",
    "\n",
    "    return csv_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>Get Total Pages</h2>\n",
    "    <p>The `get_the_total_number_of_pages` function retrieves the total number of pages from the search results. In the main function, this count of pages determines whether iterations should continue, depending on whether there are more than 50 pages to process.</p>\n",
    "    <img src=\"images/totalpages.png\" style=\"max-width: 50%; max-height: 80%; margin-left: auto; margin-right: auto;\" alt=\"Total Pages\">\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>Get Next Period Date</h2>\n",
    "    <p>The `get_date_of_next_period` function retrieves the date of the most recent article in the search results. This date is used to determine the period for the next iteration of the scraper. With each iteration, the scraper gradually approaches the specified end date.</p>\n",
    "    <img src=\"images/getdate.png\" style=\"max-width: 50%; max-height: 80%; margin-left: auto; margin-right: auto;\" alt=\"Next Period Date\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import time\n",
    "\n",
    "# Scrapes total number of pages from search results\n",
    "def get_the_total_number_of_pages(url):\n",
    "    try:\n",
    "        # Issue GET request and parse HTML\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Check status code\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            root = html.fromstring(str(soup))\n",
    "\n",
    "            # XPath to extract total pages text\n",
    "            # Handles case where no results exist\n",
    "            try:\n",
    "                paragraph_text = root.xpath('//*[@id=\"lista-risultati\"]/div/p/text()[2]')[0]\n",
    "                total_pages = paragraph_text.split()[1]\n",
    "            except:\n",
    "                total_pages = 0\n",
    "\n",
    "            # Convert to integer and return\n",
    "            return int(total_pages)\n",
    "\n",
    "        elif page.status_code == 403:\n",
    "            print(\"403 Forbidden status code encountered. Waiting for 4 minutes before retrying...\")\n",
    "\n",
    "            for i in range(4):\n",
    "                print(f\"Waiting for {i+1} minute...\")\n",
    "                time.sleep(60)\n",
    "\n",
    "            # After waiting, retry to get the total number of pages\n",
    "            return get_the_total_number_of_pages(url)\n",
    "        else:\n",
    "            print(f\"Unexpected status code: {page.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Gets date of the most recent article result\n",
    "def get_date_of_next_period(url):\n",
    "    try:\n",
    "        # Request page and parse HTML\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Check status code\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            root = html.fromstring(str(soup))\n",
    "\n",
    "            # XPath to extract last date in results set\n",
    "            dates = root.xpath('//*[@id=\"lista-risultati\"]/article/aside/a')[-1]\n",
    "            date = dates.text_content().strip()\n",
    "\n",
    "            # Return cleaned date string\n",
    "            return date\n",
    "        elif page.status_code == 403:\n",
    "            print(\"403 Forbidden status code encountered. Waiting for 4 minutes before retrying...\")\n",
    "\n",
    "            for i in range(4):\n",
    "                print(f\"Waiting for {i+1} minute...\")\n",
    "                time.sleep(60)\n",
    "\n",
    "            # After waiting, retry to get the total number of pages\n",
    "            return get_date_of_next_period(url)\n",
    "        else:\n",
    "            print(f\"Unexpected status code: {page.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>Generate Search Result Page URLs</h2>\n",
    "    <p>The `urls_generator` function generates a list of URLs for search result pages based on the specified parameters. These URLs are used to perform the search and retrieve the results for scraping.</p>\n",
    "    <img src=\"images/urlsgenerator.png\" style=\"max-width: 80%; max-height: 80%; margin-left: auto; margin-right: auto;\" alt=\"URLs Generator\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Function Description\n",
    "\n",
    "Generate a list of search result page URLs.\n",
    "\n",
    "**Parameters:**\n",
    "- *keyword* (str): The search term to be used in the URL.\n",
    "- *from_date* (str): The start date of the search period.\n",
    "- *to_date* (str): The end date of the search period.\n",
    "- *modality* (str): The mode of search (e.g., \"any\", \"all\", \"exact\").\n",
    "- *number_of_pages* (int): The total number of pages to generate URLs for.\n",
    "\n",
    "**Returns:** \n",
    "- *urls* (list): A list of URLs for search result pages.\n",
    "\n",
    "The function iterates through the specified number of pages and constructs the URL for each page based on the provided parameters. It formats the keyword string for the URL and uses placeholders for the date range, modality, and page number. The generated URLs are appended to a list and returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates list of search result page URLs\n",
    "def urls_generator(keyword, from_date, to_date, modality, number_of_pages):\n",
    "\n",
    "    # Format keyword string for URL\n",
    "    keyword = keyword.replace(\" \", \"+\")\n",
    "\n",
    "    # Base search results URL with placeholder params\n",
    "    #! Note: Please do not modify this construction as it will alter the desired behavior or result in a fatal error.\n",
    "    url_base = \"https://ricerca.repubblica.it/ricerca/repubblica?query={}&fromdate={}&todate={}&sortby=adate&author=&mode={}&page={}\"\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    # Iterate through number of pages\n",
    "    for page in range(1, number_of_pages + 1):\n",
    "\n",
    "        # Populate base URL with parameters\n",
    "        # Page number is iterated\n",
    "        url = url_base.format(keyword, from_date, to_date, modality, page)\n",
    "\n",
    "        # Append to list of URLs\n",
    "        urls.append(url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "  <div style=\"flex: 1; text-align: left;\">\n",
    "    <h2>Scraper</h2>\n",
    "    <p>The `scraper` function orchestrates the scraping process based on the provided keyword, date range, and modality.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Function Description\n",
    "\n",
    "The `scraper` function manages the scraping process by coordinating the following operations:\n",
    "\n",
    "- **Keyword**: The search term used to scrape relevant content.\n",
    "- **Date Range**: The period over which the scraping is performed, specified by the `from_date` and `to_date` parameters.\n",
    "- **Modality**: The mode of scraping, which can include web scraping or API scraping.\n",
    "\n",
    "The function begins by generating the initial set of search result URLs using the `urls_generator` function. It then determines the total number of pages for the search using the `get_the_total_number_of_pages` function.\n",
    "\n",
    "Depending on the total number of pages:\n",
    "- If the total number of pages is less than or equal to 50, the function scrapes all the URLs and saves them to a CSV file using the `create_csv` function.\n",
    "- If the total number of pages is greater than 50, the function scrapes the URLs in batches of 50. It iterates through date pagination, generating URLs for each batch and saving them to the CSV file.\n",
    "\n",
    "Upon completion, the function provides feedback indicating the successful completion of the scraping process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Main scraping function\n",
    "def scraper_one(keyword, from_date, to_date, modality):\n",
    "\n",
    "    # Generate initial set of search result URLs\n",
    "    urls = urls_generator(keyword, from_date, to_date, modality, 1)\n",
    "\n",
    "    # Get total pages for the search\n",
    "    total_number_of_pages = get_the_total_number_of_pages(urls[0])  # Example 140\n",
    "\n",
    "\n",
    "\n",
    "    # Handle no results case\n",
    "    if total_number_of_pages == 0:\n",
    "        return print(\"there is no news with the entry entered\")\n",
    "\n",
    "    # If < 50 pages, scrape all\n",
    "    elif total_number_of_pages <= 50:\n",
    "        the_urls = urls_generator(\n",
    "            keyword, from_date, to_date, modality, total_number_of_pages\n",
    "        )\n",
    "\n",
    "        # Save URLs to CSV\n",
    "        name_output = create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "        return name_output\n",
    "\n",
    "    # If > 50 pages, scrape in batches\n",
    "    else:\n",
    "        max_pages_per_keyword = 50\n",
    "\n",
    "        # First batch of 50 URLs\n",
    "        the_urls = urls_generator(\n",
    "            keyword, from_date, to_date, modality, max_pages_per_keyword\n",
    "        )\n",
    "\n",
    "        # Save CSV (create)\n",
    "        name_output = create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "        # Scrape in batches of 50 using date pagination\n",
    "        while True:\n",
    "\n",
    "            # Get next period start date\n",
    "            date_of_next_period_in_italian = get_date_of_next_period(the_urls[-1])\n",
    "            date_of_next_period = convert_Date(date_of_next_period_in_italian)\n",
    "            print(\"procces in \" + date_of_next_period)\n",
    "\n",
    "            # Generate single URL to get total pages\n",
    "            the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, 1)  # return array\n",
    "\n",
    "            # Get total pages for new date period range\n",
    "            total_number_of_pages = get_the_total_number_of_pages(the_urls[0])\n",
    "\n",
    "            # Handle total pages cases\n",
    "            if total_number_of_pages > 50:\n",
    "\n",
    "                # If more than 50 pages, generate batch\n",
    "                the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, max_pages_per_keyword)\n",
    "\n",
    "                # Save to CSV(append)\n",
    "                create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "            elif total_number_of_pages <= 50:\n",
    "\n",
    "                # If less than 50, generate all pages\n",
    "                the_urls = urls_generator(keyword, date_of_next_period, to_date, modality, total_number_of_pages)\n",
    "\n",
    "                # Save to CSV(append)\n",
    "                create_csv(keyword, from_date, to_date, the_urls)\n",
    "\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "                print(\n",
    "                    \"*********** All existing urls of the keyword were saved END ***********\"\n",
    "                )\n",
    "\n",
    "                break\n",
    "        # returns the file name for the second process\n",
    "        return name_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <body>\n",
    "    <div style=\"display: flex; align-items: center; justify-content: center\">\n",
    "      <div style=\"flex: 1; text-align: left\">\n",
    "        <h2>scraper</h2>\n",
    "        <p>\n",
    "          The `scraper` function performs scraping operations based on the\n",
    "          provided parameters:\n",
    "        </p>\n",
    "        <ul>\n",
    "          <li>\n",
    "            <strong>keyword</strong>: The search term used to scrape relevant\n",
    "            content. In this case, the keyword is \"pizza\".\n",
    "          </li>\n",
    "          <li>\n",
    "            <strong>from_date</strong>: The starting date of the search period.\n",
    "            In the example, it is set to January 1, 1984.\n",
    "          </li>\n",
    "          <li>\n",
    "            <strong>to_date</strong>: The ending date of the search period.\n",
    "            Here, it is set to January 1, 2024.\n",
    "          </li>\n",
    "          <li>\n",
    "            <strong>modality</strong>: The mode of scraping. It can be any\n",
    "            specified modality, such as \"any\".\n",
    "          </li>\n",
    "        </ul>\n",
    "      </div>\n",
    "    </div>\n",
    "    Return The name of the file where scraped data is stored.\n",
    "  </body>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_process_file = scraper(\"sopa\", '2015-01-01', \"2020-01-01\", \"any\")\n",
    "# print(file_process_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
