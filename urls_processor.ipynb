{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # Para leer y escribir archivos CSV\n",
    "import os  # Para operaciones en el sistema de archivos como verificar rutas\n",
    "import requests  # Para hacer solicitudes HTTP\n",
    "from bs4 import BeautifulSoup  # Para el análisis HTML\n",
    "import time  # Para pausas entre solicitudes\n",
    "\n",
    "\n",
    "\n",
    "# Archivo para almacenar las URL visitadas\n",
    "visited_urls_file = \"visited_urls.csv\"\n",
    "\n",
    "# Archivo CSV que contiene las URL de inicio para rastrear\n",
    "start_urls_csv = \"mafia_nigeriana_2009-02-24_2024-01-01.csv\"\n",
    "\n",
    "# Clase Spider para manejar el rastreo de artículos de noticias\n",
    "class MySpider:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Método para iniciar las solicitudes\n",
    "    def start_requests(self):\n",
    "\n",
    "        # Verifica si existe el archivo de URL visitadas para ver si es la primera ejecución o una continuación después de un fallo\n",
    "        if os.path.exists(visited_urls_file):\n",
    "            # Abre el archivo y crea un conjunto de URL ya visitadas\n",
    "            with open(visited_urls_file) as f:\n",
    "                reader = csv.reader(f)\n",
    "                visited_urls = {row[0] for row in reader if row}\n",
    "        else:\n",
    "            # Si el archivo no existe, es la primera ejecución\n",
    "            visited_urls = set()\n",
    "\n",
    "        # Abre el CSV de URL de inicio y itera a través de él\n",
    "        with open(start_urls_csv, \"r\", newline=\"\") as file:\n",
    "            reader = csv.DictReader(file)\n",
    "\n",
    "            iteration = 1\n",
    "\n",
    "            for row in reader:\n",
    "                url = row[\"URL\"]\n",
    "\n",
    "                print(f\"Iteración número {iteration}\")\n",
    "\n",
    "                # Verifica la URL contra las ya visitadas\n",
    "                if url not in visited_urls:\n",
    "                    visited_urls.add(url)\n",
    "\n",
    "                    # Hace la solicitud HTTP\n",
    "                    response = requests.get(url)\n",
    "                    if response.status_code == 200:\n",
    "                        # Llama al método parse para manejar la respuesta\n",
    "                        self.parse(response)\n",
    "                    elif response.status_code == 403:\n",
    "\n",
    "                        print('status is 403\\n' * 3)\n",
    "\n",
    "                        print('start the 4 minute wait')\n",
    "\n",
    "                        for i in range(1,5):\n",
    "                            time.sleep(60)\n",
    "                            print(f'{i} minutes have passed')\n",
    "                iteration += 1\n",
    "\n",
    "\n",
    "    # Método que maneja el análisis de la respuesta\n",
    "    def parse(self, response):\n",
    "        # Analiza el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Encuentra todos los elementos de artículo en la página\n",
    "        articles = soup.find_all(\"article\")\n",
    "\n",
    "        # Lista para almacenar los datos extraídos del artículo\n",
    "        article_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            # Obtiene el HTML interno de la etiqueta de anclaje del título\n",
    "            anchor_html = article.find(\"h1\").find(\"a\")\n",
    "\n",
    "            # Extrae el texto limpio del título\n",
    "            title = anchor_html.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Extrae el enlace del artículo\n",
    "            link = anchor_html[\"href\"]\n",
    "\n",
    "            # +++Extrae la fecha de publicación+++\n",
    "\n",
    "            # Encuentra el elemento <aside>\n",
    "            aside_element = article.find(\"aside\").find_all(\"a\")\n",
    "\n",
    "            date = aside_element[-1].get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "\n",
    "            # +++fin de extraer la fecha de publicación+++\n",
    "\n",
    "            # Agrega los datos del artículo a la lista\n",
    "            article_data.append(\n",
    "                {\"title\": title, \"link\": link, \"date\": date, \"page_url\": response.url}\n",
    "            )\n",
    "\n",
    "        # Escribe los datos extraídos del artículo en el archivo CSV\n",
    "        with open(\"output_test.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(\n",
    "                file, fieldnames=[\"title\", \"link\", \"date\", \"page_url\"]\n",
    "            )\n",
    "\n",
    "            # Escribe el encabezado si el archivo está vacío\n",
    "            if os.stat(\"output_test.csv\").st_size == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Escribe una fila para cada dato del artículo\n",
    "            for data in article_data:\n",
    "                writer.writerow(data)\n",
    "\n",
    "        # Agrega la URL al archivo de visitados\n",
    "        with open(visited_urls_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([response.url])\n",
    "\n",
    "    # Método llamado cuando se cierra el Spider\n",
    "    def closed(self, reason):\n",
    "        # Registra la razón del cierre del spider\n",
    "        print(\"Spider cerrado:\", reason)\n",
    "\n",
    "    # Método para manejar excepciones de proceso\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        if isinstance(exception, requests.exceptions.HTTPError) and exception.response.status_code == 403:\n",
    "            # Registra el error 403\n",
    "            print(f\"Recibido error 403 en {request.url}\")\n",
    "\n",
    "            # Espera 6 minutos antes de volver a intentar la solicitud\n",
    "            print(\"Esperando 6 minutos antes de volver a intentar la solicitud...\")\n",
    "            time.sleep(360)\n",
    "            return requests.get(request.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instancia del Spider\n",
    "spider = MySpider()\n",
    "# Inicia las solicitudes\n",
    "spider.start_requests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
