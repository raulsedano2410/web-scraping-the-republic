{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # For reading and writing CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import time  # For pauses between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"\\nPyarrow\", DeprecationWarning)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def search_and_separate(url_pattern, input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Filter rows containing the pattern in the 'link' column\n",
    "    rows_with_pattern = df[df[\"link\"].str.contains(url_pattern, na=False)]\n",
    "\n",
    "    # Save the filtered rows to a new CSV file\n",
    "    rows_with_pattern.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print the number of rows found\n",
    "    print(\n",
    "        f\"Found {len(rows_with_pattern)} rows containing the pattern in the 'link' column.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59519 rows containing the pattern in the 'link' column.\n"
     ]
    }
   ],
   "source": [
    "url_pattern = \"https://quotidiano.repubblica.it/edicola/searchdetail\\?id=http://archivio.repubblica.extra.kataweb.it/\"\n",
    "\n",
    "input_file = 'output_part1.csv'\n",
    "\n",
    "output_file = \"url_patron_1.csv\"\n",
    "\n",
    "search_and_separate(url_pattern, input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(response, visited_links, visited_links_file):\n",
    "  # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "    article = soup.find(\"article\")\n",
    "\n",
    "    title = article.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # Extract the article link\n",
    "    content = article.find(\"p\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "    date = article.find(\"aside\").find(\"a\").find(\"time\").get_text(separator=\" \",strip=True)\n",
    "\n",
    "    article_data = [\n",
    "         {\"title\": title, \"content\": content, \"date\": date, \"src_url\": response.url}\n",
    "    ]\n",
    "\n",
    "    # Define los nombres de los campos\n",
    "    fieldnames = [\"title\", \"content\", \"date\", \"src_url\"]\n",
    "\n",
    "    # Verifica si el archivo existe\n",
    "    mode = \"a\" if os.path.exists(\"output\") else \"w\"\n",
    "        # Si el archivo existe, abre en modo \"append\"\n",
    "    with open(\"output\", \"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        # Escribe los datos del artículo en el archivo\n",
    "        writer.writerows(article_data)\n",
    "\n",
    "\n",
    "    # else:\n",
    "    #     # Si el archivo no existe, abre en modo de escritura normal\n",
    "    #     with open(\"output\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    #         writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    #         # Escribe los datos del artículo en el archivo\n",
    "    #         writer.writerows(article_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # For reading and writing CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import time  # For pauses between requests\n",
    "\n",
    "def scraper(visited_links_file, start_links_csv):\n",
    "    # Comprobar si el archivo de URL visitadas existe para ver si es la primera ejecución o una continuación después de un error\n",
    "    if os.path.exists(visited_links_file):\n",
    "       # Abra el archivo y cree un conjunto de URL visitadas\n",
    "        with open(visited_links_file) as f:\n",
    "            reader = csv.reader(f)\n",
    "            visited_links = {row[0] for row in reader if row}\n",
    "    else:\n",
    "        # If the file doesn't exist, it's the first run\n",
    "        visited_links = set()\n",
    "\n",
    "    # Open the start URL CSV and iterate through it\n",
    "    with open(f\"./{start_links_csv}\", \"r\", newline=\"\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        iteration = 1\n",
    "\n",
    "        for row in reader:\n",
    "            link = row[\"link\"]\n",
    "\n",
    "            pattern = \"https://quotidiano.repubblica.it/edicola/searchdetail\\?id=http://archivio.repubblica.extra.kataweb.it/\"\n",
    "\n",
    "            if pattern in link:\n",
    "\n",
    "                print(f\"iteration number {iteration}\")\n",
    "\n",
    "                # Check the link against the already visited ones\n",
    "                if link not in visited_links:\n",
    "                    visited_links.add(link)\n",
    "\n",
    "                    # Make the HTTP request\n",
    "                    response = requests.get(link)\n",
    "                    if response.status_code == 200:\n",
    "                        # Call the parse method to handle the response\n",
    "                        parse(response, visited_links, visited_links_file)\n",
    "                    elif response.status_code == 403:\n",
    "\n",
    "                        print(\"status is 403\\n\" * 3)\n",
    "\n",
    "                        print(\"start the 4 minute wait\")\n",
    "\n",
    "                        # Wait for 4 minutes before retrying\n",
    "                        for i in range(1, 5):\n",
    "                            time.sleep(60)\n",
    "                            print(f\"{i} minutes have passed\")\n",
    "                iteration += 1\n",
    "\n",
    "            print(\n",
    "                f\"Check the result: xxxxx\"\n",
    "            )\n",
    "            print(\"************** End **************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Method to handle parsing of the response\n",
    "    def parse(self, response):\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all article elements on the page\n",
    "        articles = soup.find_all(\"article\")\n",
    "\n",
    "        # List to store extracted article data\n",
    "        article_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            # Get the inner HTML of the title anchor tag\n",
    "            anchor_html = article.find(\"h1\").find(\"a\")\n",
    "\n",
    "            # Extract clean text from the title\n",
    "            title = anchor_html.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Extract the article link\n",
    "            link = anchor_html[\"href\"]\n",
    "\n",
    "            # Extract the publication date\n",
    "            aside_element = article.find(\"aside\").find_all(\"a\")\n",
    "            date = aside_element[-1].get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Add article data to the list\n",
    "            article_data.append(\n",
    "                {\"title\": title, \"link\": link, \"date\": date, \"page_url\": response.url}\n",
    "            )\n",
    "\n",
    "        # Write the extracted article data to the CSV file\n",
    "        with open(\n",
    "            self._build_visited_urls_file(use_output_suffix=True),\n",
    "            \"a\",\n",
    "            newline=\"\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as file:\n",
    "            writer = csv.DictWriter(\n",
    "                file, fieldnames=[\"title\", \"link\", \"date\", \"page_url\"]\n",
    "            )\n",
    "\n",
    "            # Write the header if the file is empty\n",
    "            if (\n",
    "                os.stat(self._build_visited_urls_file(use_output_suffix=True)).st_size\n",
    "                == 0\n",
    "            ):\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Write a row for each article data\n",
    "            for data in article_data:\n",
    "                writer.writerow(data)\n",
    "\n",
    "        # Add the URL to the visited file\n",
    "        with open(self.visited_urls_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([response.url])\n",
    "\n",
    "    def _build_visited_urls_file(self, use_output_suffix=False):\n",
    "\n",
    "        # Get the base filename of the start URLs CSV file without the extension\n",
    "        base_filename = os.path.splitext(os.path.basename(self.start_urls_csv))[0]\n",
    "\n",
    "        # Remove the date part from the base filename\n",
    "        base_filename_without_date = base_filename.split(\"_\")[0]\n",
    "\n",
    "        # Construct the filename suffix based on the use_output_suffix parameter\n",
    "        if use_output_suffix:\n",
    "            filename_suffix = \"_output.csv\"\n",
    "        else:\n",
    "            filename_suffix = \"_visited_urls.csv\"\n",
    "\n",
    "        # Construct the filename for the visited URLs file\n",
    "        visited_urls_filename = f\"{base_filename_without_date}{filename_suffix}\"\n",
    "\n",
    "        return f\"./output/{visited_urls_filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "https://quotidiano.repubblica.it/edicola/searchdetail?id=http://archivio.repubblica.extra.kataweb.it/archivio/repubblica\n",
    "/2019/11/02/incosciente-non-fu-stupro-pena-ridotta-per-il-protesta18.html&hl=&query=sopa&field=nel+testo&testata=repubblica&newspaper=REP&edition=nazionale&zona=sfoglio&ref=search\n",
    "\n",
    "\n",
    "\n",
    "https://ricerca.repubblica.it/repubblica/archivio/repubblica\n",
    "https://ricerca.repubblica.it/repubblica/archivio/repubblica/2006/08/sezioni/cronaca/milano-ladri-bambini/milano-ladri-bambini/milano-ladri-bambini.html?ref=search\n",
    "\n",
    "\n",
    "\n",
    "https://ricerca.repubblica.it/repubblica/archivio/repubblica/2019/11/02/incosciente-non-fu-stupro-pena-ridotta-per-il-protesta18.html&hl=&query=sopa&field=nel+testo&testata=repubblica&newspaper=REP&edition=nazionale&zona=sfoglio&ref=search\n",
    "\n",
    "\n",
    "http://www.repubblica.it/sport/vari/2015/06/25/news/giochi_europei_quattro_azzurri_in_finale_per_la_boxe-117672858/?ref=search\n",
    "\n",
    "\n",
    "http://www.repubblica.it/2006/08/sezioni/cronaca/milano-ladri-bambini/mila   no-ladri-bambini/milano-ladri-bambini.html?ref=search\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # For reading and writing CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import time  # For pauses between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
